# @package _global_

# to execute this experiment run:
# python run.py experiment=example

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: gnn.yaml
  - override /datamodule: adaptive.yaml
  - override /callbacks: default.yaml
  - override /logger:
    - neptune.yaml
#    - mlflow.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# can also be accessed by loggers
name: "adaptive-sampler-1"

seed: 123

trainer:
  min_epochs: 0
  max_epochs: 20
#  gradient_clip_val: 0.5
  gpus: 0
#  strategy: ddp

datamodule:
#  num_workers: 0
#  persistent_workers: False

  # dataset
  dataset: cora
  batch_size: 70
  undirected: False
  # training
  lr: 0.01
  weight_decay: 1e-4
  # adapt
  node_budget: 30
  alpha: 0.1
  max_hop: 10
  min_nodes: 70
  num_groups: 1
  group_type: full
  ego_mode: False
  to_single_layer: False

model:
  model: sage
  lr: 0.001
  # adapt
  subg_pool: []
  pool_type: mean
  # base
  hidden_channels: 128
  conv_layers: 2
  dropedge: 0
  residual: null
  jk: null



# 覆盖掉logger配置
#logger:
#  mlflow:
#    run_name: run3  # 指定实验名

#logger:
#  csv:
#    name: csv/${name}
#  wandb:
#    tags: ["mnist", "simple_dense_net"]