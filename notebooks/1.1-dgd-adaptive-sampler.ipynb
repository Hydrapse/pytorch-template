{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[89250, 500], edge_index=[2, 899756], y=[89250], train_mask=[89250], val_mask=[89250], test_mask=[89250])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "from torch_geometric.datasets import Flickr\n",
    "import torch\n",
    "\n",
    "dataset = Flickr(\"/mnt/nfs-ssd/raw-datasets/pyg-format/Flickr\")\n",
    "data = dataset[0]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.utils import *\n",
    "\n",
    "l_edge = data.edge_index\n",
    "# l_edge, _ = add_self_loops(data.edge_index)\n",
    "contains_self_loops(l_edge)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "w_ego_root = nn.Parameter(torch.Tensor(data.num_features))\n",
    "w_ego_u = nn.Parameter(torch.Tensor(data.num_features))\n",
    "w_layer_v = nn.Parameter(torch.Tensor(data.num_features, 1))\n",
    "w_layer_u = nn.Parameter(torch.Tensor(data.num_features, 1))\n",
    "w_threshold = nn.Parameter(torch.Tensor(data.num_features, 1))\n",
    "\n",
    "nn.init.constant_(w_ego_root, 1)\n",
    "nn.init.constant_(w_ego_u, 1)\n",
    "nn.init.constant_(w_layer_v, 1)\n",
    "nn.init.constant_(w_layer_u, 1)\n",
    "nn.init.constant_(w_threshold, 1e-5)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "\n",
    "def ego_kernel(h_root, h_u):\n",
    "    h_root = h_root * w_ego_root\n",
    "    h_u = h_u * w_ego_u\n",
    "\n",
    "    return cos(h_root, h_u)\n",
    "\n",
    "\n",
    "def layer_kernel(h_v, h_u, adj):\n",
    "    h_v = h_v @ w_layer_v\n",
    "    h_u = h_u @ w_layer_u\n",
    "    h_msg = adj @ h_v\n",
    "\n",
    "    return F.normalize(F.relu(h_msg + h_u), dim = 0).view(-1)\n",
    "\n",
    "\n",
    "def node_importance(self_adj_t):\n",
    "    adj_t = self_adj_t.fill_value(1., dtype=torch.float)\n",
    "    adj_t = adj_t.set_diag()\n",
    "    deg = adj_t.sum(dim=1).to(torch.float)\n",
    "    deg_inv_sqrt = deg.pow(-1)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "    return adj_t.sum(dim=1).pow(0.5)\n",
    "\n",
    "\n",
    "def edge_global_to_batch(n_idx, e_idx, undirected = False):\n",
    "    \"\"\"\n",
    "     将全局edgeId转换成batch内id\n",
    "    \"\"\"\n",
    "    batch_idx = []\n",
    "    for edge in e_idx.t():\n",
    "        batch_idx.append([\n",
    "            (n_idx == edge[0]).nonzero(as_tuple=True)[0],\n",
    "            (n_idx == edge[1]).nonzero(as_tuple=True)[0]\n",
    "        ])\n",
    "\n",
    "    batch_idx = torch.tensor(batch_idx).t()\n",
    "    batch_idx, _ = add_remaining_self_loops(batch_idx)\n",
    "    if undirected:\n",
    "        batch_idx = to_undirected(batch_idx)\n",
    "\n",
    "    val, idx = torch.sort(batch_idx[0])\n",
    "    return torch.stack([val, batch_idx[1][idx]])\n",
    "\n",
    "\n",
    "def get_node_p(n_idx, p_dict, method = 'sum'):\n",
    "    \"\"\"\n",
    "        methods: [sum, mean, max]\n",
    "    \"\"\"\n",
    "    p_arr = []\n",
    "    for nid in n_idx:\n",
    "        p_node = p_dict[nid.item()]\n",
    "        assert p_node != [], nid\n",
    "        if method == 'sum':\n",
    "            p_node = sum(p_node)\n",
    "        elif method == 'mean':\n",
    "            p_node = sum(p_node) / len(p_node)\n",
    "        elif method == 'max':\n",
    "            p_node = max(p_node)\n",
    "        else:\n",
    "            raise NotImplementedError(method+ \"not implemented!\")\n",
    "        p_arr.append(p_node.view(1,1))\n",
    "\n",
    "    return torch.cat(p_arr).view(-1)\n",
    "\n",
    "\n",
    "class EgoData(Data):\n",
    "    def __init__(self, x, edge_index, y, p=None):\n",
    "        y = y[0] if y.numel() > 1 else y\n",
    "        super().__init__(x, edge_index, y=y)\n",
    "        # self.ego_index = ego_index\n",
    "        self.p = p\n",
    "\n",
    "    # def __inc__(self, key, value, *args, **kwargs):\n",
    "    #     if key == 'ego_ptr':\n",
    "    #         return self.num_nodes\n",
    "    #     else:\n",
    "    #         return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "def unique(x, dim=-1):\n",
    "    unique, inverse = torch.unique(x, return_inverse=True, dim=dim)\n",
    "    perm = torch.arange(inverse.size(dim), dtype=inverse.dtype, device=inverse.device)\n",
    "    inverse, perm = inverse.flip([dim]), perm.flip([dim])\n",
    "    return unique, inverse.new_empty(unique.size(dim)).scatter_(dim, inverse, perm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.92 s, sys: 129 ms, total: 8.05 s\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch_sparse import SparseTensor\n",
    "from collections import defaultdict\n",
    "from torch_scatter import scatter\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "row, col = l_edge.cpu()\n",
    "self_adj_t = SparseTensor(\n",
    "    row=row, col=col, value=torch.arange(col.size(0)),\n",
    "    sparse_sizes=(data.num_nodes, data.num_nodes)).t()\n",
    "\n",
    "x = data.x\n",
    "n_imp = node_importance(self_adj_t)\n",
    "\n",
    "\n",
    "# 入参\n",
    "self_budget = 200\n",
    "a = 0.5\n",
    "p_gather = 'sum'\n",
    "# margin_expand = False  # 每一轮只用边缘节点探索\n",
    "\n",
    "for batch_i in range(1):\n",
    "    batch_node = torch.tensor([13687], dtype=torch.int64)\n",
    "\n",
    "    e_id = []  # 存储所有边index\n",
    "    n_id = [batch_node] # 存储所有点index\n",
    "    # n_p = defaultdict(list)  # 储存所有点权重\n",
    "    n_p = [torch.tensor([1.])]\n",
    "\n",
    "    ego_store = defaultdict(float)\n",
    "\n",
    "    # def get_ego_score(u):\n",
    "    #     s_arr = []\n",
    "    #     for u_id in list(u):\n",
    "    #         score = ego_store[u_id]\n",
    "    #         score = ego_kernel(x[batch_node], x[u_id]) if score == 0.0 else score\n",
    "    #         s_arr.append(score)\n",
    "    #     return torch.cat(s_arr)\n",
    "\n",
    "    # 初始化\n",
    "    # n_p[batch_node.item()] = [torch.tensor(1)]\n",
    "    r_max = (x[batch_node] @ w_threshold).view(-1)\n",
    "    p_norm = float('-inf')\n",
    "    v = batch_node\n",
    "    budget = self_budget\n",
    "    hop = 0\n",
    "    while budget > 0:\n",
    "        \"\"\"\n",
    "        只采节点不采边\n",
    "        \"\"\"\n",
    "        adj_t, u = self_adj_t.sample_adj(v, -1, replace=False)\n",
    "        u_size = u.size(-1)\n",
    "        row, col, layer_e = adj_t.coo()\n",
    "        adj = SparseTensor(row=col, col=row, sparse_sizes=adj_t.sparse_sizes()[::-1])\n",
    "\n",
    "        \"\"\"计算p_u\"\"\"\n",
    "        # ego_score = get_ego_score(u)\n",
    "        ego_score = ego_kernel(x[batch_node], x[u])\n",
    "        layer_score = layer_kernel(x[v], x[u], adj)\n",
    "        p_u = (a * ego_score + (1 - a) * layer_score) \\\n",
    "              * n_imp[u] \\\n",
    "              * (budget / self_budget)\n",
    "        # p_norm = max(torch.max(p_u).detach(), p_norm)\n",
    "        if hop == 0: p_norm = p_u[0].clone().detach()\n",
    "        p_u /= p_norm\n",
    "\n",
    "        \"\"\"计算mask\"\"\"\n",
    "        p_clip = torch.clamp(p_u, min = 0, max=1)\n",
    "        num_sample = torch.clamp(p_clip.sum(), 1, u_size).int()\n",
    "        num_sample = min(budget, num_sample)\n",
    "        mask = torch.zeros((u_size,), dtype=torch.bool)\n",
    "        mask[torch.multinomial(p_clip, num_sample)] = 1\n",
    "        budget -= num_sample\n",
    "        # mask = torch.bernoulli(torch.clamp(p_u, min=0, max=1)).to(torch.bool)\n",
    "        # layer_cost = sum(mask).item()\n",
    "        # if layer_cost > budget:\n",
    "        #     _, p_id = torch.sort(p_u[mask], dim=-1, descending=True)\n",
    "        #     mask = torch.zeros_like(mask)\n",
    "        #     mask[p_id[:budget]] = 1\n",
    "        # budget -= layer_cost\n",
    "\n",
    "        p_u -= r_max  # 为了让r_max可导\n",
    "        mask = mask & (p_u > 0)\n",
    "        if sum(mask).item() < 1: break\n",
    "\n",
    "        _, col, layer_e = adj_t.coo()\n",
    "        edges = [layer_e[col[:] == i] for i in torch.arange(u_size)[mask]]\n",
    "        e_id.append(torch.cat(edges))\n",
    "\n",
    "        sampled_u = u[mask]\n",
    "        # margin_u = torch.tensor(list(set(sampled_u.numpy()).difference(set(n_id.numpy()))),dtype=n_id.dtype)\n",
    "        # n_id = torch.cat((n_id, margin_u))\n",
    "        n_id.append(sampled_u)\n",
    "        n_p.append(p_u[mask])\n",
    "\n",
    "        # for idx, p_val in zip(sampled_u.tolist(), p_u[mask]):\n",
    "        #     n_p[idx].append(p_val)\n",
    "\n",
    "        # v = margin_u if margin_expand else sampled_u\n",
    "        v = sampled_u\n",
    "        hop += 1\n",
    "\n",
    "    e_id = torch.cat(e_id).unique()\n",
    "    n_id, n_mask = torch.cat(n_id).unique(return_inverse=True)\n",
    "    # n_id, n_mask = unique(torch.cat(n_id))\n",
    "    p = scatter(torch.cat(n_p), n_mask, dim=-1, reduce=p_gather)\n",
    "\n",
    "    # n_id[[0, n_mask[0]]] = n_id[[n_mask[0], 0]]\n",
    "    batch_edge = edge_global_to_batch(n_id, l_edge[:, e_id])\n",
    "    # p = n_p[n_mask]\n",
    "    # p = get_node_p(n_id, n_p, 'sum')\n",
    "\n",
    "\n",
    "    ego_data = EgoData(x[n_id], batch_edge, data.y[n_id[n_mask[0]]], p)\n",
    "    ego_data.hop = hop\n",
    "    ego_data.ego_ptr = n_mask[0]\n",
    "\n",
    "    # print(\"hop_num:\", hop, ego_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 571, 4104])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_edge[:, 45090]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_data.p[0].backward()\n",
    "w_layer_v.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1497,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-4.6185e-08],\n        [-7.3994e-08],\n        [ 3.3367e-08],\n        [-7.9778e-08],\n        [ 3.3293e-08],\n        [ 1.5478e-07],\n        [ 7.8158e-10],\n        [-5.6782e-08],\n        [-3.7173e-08],\n        [-4.8598e-08],\n        [-6.1097e-09],\n        [ 2.7541e-08],\n        [-9.2305e-08],\n        [-8.9921e-08],\n        [ 8.5099e-08],\n        [ 1.8860e-08],\n        [ 6.3238e-08],\n        [-6.5001e-08],\n        [ 3.3741e-08],\n        [-8.7589e-08],\n        [ 2.3794e-08],\n        [ 7.7014e-08],\n        [-6.9723e-08],\n        [ 1.0773e-08],\n        [ 1.0246e-07],\n        [-1.2371e-08],\n        [-6.3099e-08],\n        [-4.7066e-08],\n        [-8.9762e-08],\n        [-7.9280e-08],\n        [ 6.3772e-08],\n        [-6.7350e-08],\n        [ 2.2183e-08],\n        [ 3.4333e-09],\n        [ 1.6413e-08],\n        [ 3.4389e-08],\n        [-9.0412e-08],\n        [-4.0481e-08],\n        [-4.2033e-08],\n        [-7.5425e-08],\n        [ 4.5143e-08],\n        [-1.1203e-07],\n        [ 7.0886e-08],\n        [ 8.0741e-08],\n        [ 5.1556e-09],\n        [ 1.4584e-08],\n        [-1.8507e-08],\n        [-5.5162e-08],\n        [-4.2840e-09],\n        [-8.3468e-08],\n        [-6.6888e-08],\n        [-6.3364e-08],\n        [ 9.3902e-08],\n        [ 7.2722e-08],\n        [ 2.3111e-08],\n        [-7.1636e-08],\n        [ 1.2625e-08],\n        [ 1.1795e-07],\n        [-9.2594e-08],\n        [ 1.1195e-07],\n        [ 2.3715e-09],\n        [-3.7263e-08],\n        [ 6.2974e-08],\n        [-5.7243e-08],\n        [-3.8200e-08],\n        [ 2.9471e-08],\n        [-8.4293e-08],\n        [ 2.0915e-08],\n        [-4.8982e-08],\n        [ 5.9655e-08],\n        [ 1.9728e-09],\n        [-4.7076e-08],\n        [-7.0082e-08],\n        [ 1.3194e-07],\n        [-4.2823e-08],\n        [-6.9145e-08],\n        [ 1.5811e-08],\n        [-6.7593e-08],\n        [ 4.3292e-09],\n        [ 1.1614e-07],\n        [ 1.2999e-07],\n        [-6.4014e-08],\n        [-8.3253e-08],\n        [-8.7087e-08],\n        [-6.4667e-08],\n        [ 1.5607e-08],\n        [ 5.6874e-09],\n        [ 5.9964e-08],\n        [ 1.0583e-08],\n        [-3.4817e-09],\n        [ 4.2225e-08],\n        [ 3.2138e-10],\n        [-6.7268e-08],\n        [ 1.6111e-07],\n        [ 5.6485e-08],\n        [-7.3823e-08],\n        [ 3.8120e-08],\n        [ 2.9776e-08],\n        [ 7.1268e-08],\n        [-5.5864e-08],\n        [-2.0892e-08],\n        [ 7.9960e-08],\n        [ 5.9177e-08],\n        [ 1.0740e-07],\n        [ 6.3355e-08],\n        [ 7.1986e-09],\n        [-8.3067e-08],\n        [-5.2893e-08],\n        [ 4.9928e-08],\n        [-7.7628e-08],\n        [ 8.3095e-08],\n        [ 2.2554e-08],\n        [-4.6985e-08],\n        [-5.8789e-08],\n        [-5.7319e-08],\n        [ 4.5648e-08],\n        [ 1.0336e-07],\n        [ 4.4257e-08],\n        [ 1.5979e-08],\n        [ 8.1235e-08],\n        [ 1.2050e-08],\n        [-2.5299e-08],\n        [-8.4566e-08],\n        [-6.1442e-08],\n        [ 4.0159e-09],\n        [-4.1610e-08],\n        [ 2.6261e-08],\n        [-3.7120e-08],\n        [-6.7136e-08],\n        [-5.9346e-08],\n        [ 3.7964e-08],\n        [-5.4840e-08],\n        [-1.1023e-08],\n        [ 5.0005e-08],\n        [-6.4599e-08],\n        [-1.0593e-07],\n        [ 7.7888e-09],\n        [-5.0751e-08],\n        [-4.2606e-08],\n        [-4.6922e-08],\n        [ 4.0267e-08],\n        [-1.3420e-09],\n        [ 1.3405e-07],\n        [-7.6853e-08],\n        [-4.9310e-09],\n        [-6.0775e-08],\n        [ 1.0482e-07],\n        [ 6.3372e-09],\n        [-1.7058e-08],\n        [ 9.8938e-08],\n        [ 3.8392e-08],\n        [-7.0490e-08],\n        [ 1.1787e-08],\n        [-1.1616e-07],\n        [-1.7698e-08],\n        [-6.2461e-08],\n        [-6.0326e-08],\n        [ 1.5037e-07],\n        [-5.0296e-08],\n        [-5.2682e-08],\n        [-5.3502e-08],\n        [-8.0859e-08],\n        [ 9.3108e-08],\n        [-3.4741e-08],\n        [ 1.4222e-08],\n        [-6.8837e-08],\n        [-4.2030e-08],\n        [-5.3535e-08],\n        [-5.2778e-08],\n        [ 8.6851e-08],\n        [-3.1233e-08],\n        [-5.4100e-08],\n        [-4.2162e-08],\n        [ 9.5412e-08],\n        [ 1.9015e-07],\n        [ 5.0149e-08],\n        [ 3.0561e-08],\n        [-6.4229e-08],\n        [ 2.5133e-07],\n        [-3.7205e-08],\n        [-2.7661e-08],\n        [ 4.9527e-08],\n        [ 3.0995e-07],\n        [-4.2751e-08],\n        [ 5.7028e-08],\n        [-9.2578e-08],\n        [-1.0447e-07],\n        [-8.4697e-08],\n        [-7.7543e-09],\n        [ 7.5228e-08],\n        [-4.7651e-08],\n        [-3.5764e-08],\n        [ 2.1967e-08],\n        [ 1.4746e-07],\n        [ 1.9995e-07],\n        [-7.3166e-09],\n        [-5.6819e-08],\n        [ 3.7058e-08],\n        [-9.6548e-08],\n        [-4.1735e-08],\n        [-6.1567e-08],\n        [ 1.3094e-08],\n        [ 3.1011e-09],\n        [ 1.3070e-07],\n        [-6.4506e-08],\n        [-4.9334e-08],\n        [ 7.9612e-08],\n        [-3.3324e-08],\n        [-3.8059e-08],\n        [-1.0452e-07],\n        [ 2.0543e-07],\n        [ 7.8044e-08],\n        [-7.7308e-08],\n        [-3.3899e-08],\n        [-6.2585e-08],\n        [-4.1421e-08],\n        [-6.6695e-08],\n        [-9.4313e-08],\n        [ 1.1876e-08],\n        [-1.1008e-07],\n        [ 1.9876e-08],\n        [-7.2893e-08],\n        [ 1.6830e-08],\n        [-5.0508e-08],\n        [ 1.8103e-08],\n        [-6.2613e-08],\n        [-1.9471e-08],\n        [-1.4959e-08],\n        [ 6.8682e-08],\n        [ 1.1383e-08],\n        [ 5.1821e-08],\n        [ 1.0905e-08],\n        [-2.7232e-08],\n        [-1.0028e-08],\n        [-2.3248e-08],\n        [-2.0899e-08],\n        [ 9.0299e-10],\n        [-4.2784e-08],\n        [ 1.7934e-07],\n        [-9.1559e-08],\n        [-1.0835e-07],\n        [ 1.8164e-08],\n        [-4.3898e-08],\n        [-5.0886e-09],\n        [ 4.9479e-09],\n        [-3.0232e-08],\n        [-6.2795e-08],\n        [-2.0561e-08],\n        [-6.3051e-08],\n        [ 7.1434e-08],\n        [ 5.3732e-08],\n        [-1.6757e-08],\n        [ 1.3163e-07],\n        [-5.7237e-08],\n        [-4.5914e-08],\n        [ 1.7619e-08],\n        [-7.2493e-08],\n        [ 3.4959e-07],\n        [ 1.1916e-08],\n        [ 4.4306e-09],\n        [-9.0643e-08],\n        [ 3.1240e-08],\n        [ 2.0262e-09],\n        [ 1.5413e-07],\n        [-9.9605e-09],\n        [-7.6883e-08],\n        [-4.0877e-09],\n        [ 1.4009e-07],\n        [ 7.6588e-08],\n        [-9.5897e-08],\n        [-8.3793e-09],\n        [-3.9461e-09],\n        [-1.2662e-07],\n        [ 7.3395e-08],\n        [ 4.9277e-08],\n        [-8.0326e-08],\n        [ 1.7669e-07],\n        [-8.7393e-10],\n        [-1.4014e-07],\n        [-6.9841e-08],\n        [-3.2760e-07],\n        [-8.3140e-08],\n        [-8.8045e-08],\n        [-6.7276e-08],\n        [ 7.5775e-08],\n        [-1.1705e-07],\n        [-1.0066e-08],\n        [-4.8489e-08],\n        [ 2.1823e-08],\n        [-5.2428e-08],\n        [ 4.5583e-08],\n        [ 5.7480e-08],\n        [-5.0835e-08],\n        [-3.0624e-08],\n        [ 7.3777e-08],\n        [-2.2030e-08],\n        [ 2.0186e-08],\n        [ 5.4703e-08],\n        [ 4.2390e-08],\n        [-5.0569e-08],\n        [ 4.4837e-08],\n        [ 2.4731e-07],\n        [ 3.8643e-08],\n        [-2.1139e-08],\n        [-7.1570e-08],\n        [ 5.0299e-08],\n        [-1.0883e-08],\n        [ 1.2274e-07],\n        [-5.5916e-08],\n        [ 2.7454e-09],\n        [ 4.4034e-08],\n        [-4.1486e-08],\n        [-5.6129e-08],\n        [-7.5763e-08],\n        [ 2.2574e-08],\n        [-5.1360e-08],\n        [ 1.1340e-07],\n        [ 6.1552e-09],\n        [ 9.8399e-09],\n        [-6.0112e-10],\n        [-1.3317e-07],\n        [ 3.5198e-08],\n        [-4.3162e-08],\n        [ 6.9534e-08],\n        [ 8.2340e-08],\n        [-4.6381e-08],\n        [ 5.6080e-08],\n        [-1.3086e-07],\n        [ 5.8865e-09],\n        [ 1.1711e-07],\n        [-8.2054e-08],\n        [-8.6087e-08],\n        [ 8.0352e-09],\n        [-7.0522e-08],\n        [ 5.7424e-08],\n        [-3.4638e-08],\n        [ 6.6249e-08],\n        [-5.3501e-08],\n        [ 4.1782e-07],\n        [ 7.7601e-08],\n        [-3.5694e-09],\n        [-6.3673e-09],\n        [-4.5692e-08],\n        [-2.5071e-08],\n        [ 4.3796e-09],\n        [ 2.4103e-07],\n        [ 1.8768e-07],\n        [-5.3260e-08],\n        [-2.6094e-08],\n        [ 1.3712e-07],\n        [-6.9944e-08],\n        [-5.7445e-08],\n        [-7.1713e-08],\n        [-9.2504e-08],\n        [-6.2137e-08],\n        [-1.2332e-08],\n        [ 4.0667e-08],\n        [ 1.1453e-09],\n        [ 5.6470e-08],\n        [ 9.7981e-08],\n        [-1.6122e-09],\n        [-1.6300e-09],\n        [-7.4319e-08],\n        [ 9.7941e-08],\n        [ 2.4317e-09],\n        [-1.4665e-09],\n        [-5.3716e-08],\n        [-1.0060e-08],\n        [ 1.6741e-07],\n        [-5.5687e-08],\n        [-8.4067e-08],\n        [ 5.8801e-08],\n        [-2.7414e-08],\n        [-6.3092e-08],\n        [ 6.3611e-08],\n        [-2.7357e-07],\n        [ 3.9356e-08],\n        [-7.2118e-08],\n        [ 4.4039e-08],\n        [-3.3355e-08],\n        [ 1.5299e-08],\n        [-5.7244e-08],\n        [-6.3138e-08],\n        [-8.2363e-08],\n        [-5.5580e-08],\n        [-4.6045e-08],\n        [ 1.6029e-08],\n        [-3.8537e-08],\n        [-1.5190e-09],\n        [ 5.1461e-10],\n        [-6.8582e-08],\n        [ 4.1194e-09],\n        [ 6.5050e-08],\n        [-5.6064e-09],\n        [ 1.1939e-08],\n        [-8.4110e-08],\n        [ 1.4279e-08],\n        [-6.1015e-08],\n        [-1.9299e-08],\n        [ 1.2305e-07],\n        [-2.7902e-09],\n        [-5.6387e-08],\n        [ 3.8621e-09],\n        [ 0.0000e+00],\n        [-2.9211e-08],\n        [ 7.3574e-08],\n        [ 3.1926e-08],\n        [-3.8298e-09],\n        [-5.2555e-08],\n        [ 1.6049e-08],\n        [-3.4884e-08],\n        [-6.9376e-08],\n        [-9.5887e-08],\n        [-5.8601e-08],\n        [-6.3428e-08],\n        [ 3.7814e-07],\n        [ 3.8710e-08],\n        [-4.5059e-09],\n        [-7.2871e-08],\n        [-4.1576e-08],\n        [ 8.1357e-08],\n        [ 5.9524e-08],\n        [-6.3722e-08],\n        [ 4.3801e-08],\n        [ 5.0878e-08],\n        [-6.8585e-08],\n        [-6.6283e-08],\n        [-5.7035e-08],\n        [-2.4242e-08],\n        [ 3.6844e-07],\n        [-8.1174e-08],\n        [ 8.2198e-08],\n        [ 4.3626e-08],\n        [-5.4924e-08],\n        [-1.4350e-08],\n        [-5.0724e-08],\n        [-4.4240e-08],\n        [ 2.1094e-08],\n        [ 8.6224e-08],\n        [ 2.8231e-09],\n        [-4.9150e-08],\n        [-4.9672e-09],\n        [ 7.1575e-08],\n        [-4.3898e-08],\n        [-1.5121e-07],\n        [ 1.6209e-08],\n        [ 1.0946e-07],\n        [ 1.1838e-07],\n        [ 9.1666e-08],\n        [ 3.8607e-08],\n        [-4.5900e-08],\n        [ 1.6948e-07],\n        [-1.0662e-08],\n        [ 4.1619e-08],\n        [-5.7887e-08],\n        [-4.7294e-08],\n        [-7.7274e-08],\n        [-7.2908e-08],\n        [-9.4359e-08],\n        [-9.6309e-08],\n        [ 3.5161e-08],\n        [-7.5232e-08],\n        [ 9.1288e-08],\n        [-1.0250e-07],\n        [ 2.1422e-07],\n        [ 7.2448e-08],\n        [-5.2267e-08],\n        [-4.2977e-08],\n        [-6.1598e-08],\n        [-2.8025e-08],\n        [ 2.8712e-07],\n        [ 1.0947e-07],\n        [-8.0980e-08],\n        [ 1.6962e-08],\n        [ 9.7183e-08],\n        [-6.9798e-08],\n        [ 9.0927e-08],\n        [ 1.1416e-08],\n        [ 2.1901e-07],\n        [-7.0700e-08],\n        [-3.5162e-08],\n        [-2.1720e-08],\n        [ 3.4928e-08],\n        [ 1.0542e-08],\n        [-1.6675e-07],\n        [ 6.4131e-08],\n        [-4.5413e-09],\n        [ 1.8770e-08],\n        [ 1.4368e-08],\n        [ 7.0426e-08],\n        [-1.1593e-08],\n        [-3.2210e-08],\n        [ 5.7616e-08],\n        [ 1.1984e-07],\n        [ 2.6039e-08],\n        [ 1.4742e-08],\n        [-8.9091e-08],\n        [-6.9927e-08],\n        [ 7.9506e-09],\n        [-6.9227e-08]])"
     },
     "execution_count": 1497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numel'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-146b6e076527>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfilter_data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mbatch_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_data_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mego_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mego_data\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# batch_data = filter_data(data, n_id, batch_edge[0], batch_edge[1], e_id)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch_geometric/data/batch.py\u001B[0m in \u001B[0;36mfrom_data_list\u001B[0;34m(cls, data_list, follow_batch, exclude_keys)\u001B[0m\n\u001B[1;32m     66\u001B[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m         batch, slice_dict, inc_dict = collate(\n\u001B[0m\u001B[1;32m     69\u001B[0m             \u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0mdata_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_list\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch_geometric/data/collate.py\u001B[0m in \u001B[0;36mcollate\u001B[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcls\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mdata_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_base_cls\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# Dynamic inheritance.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch_geometric/data/batch.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDynamicInheritance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew_cls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-2929d9b9a728>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, x, edge_index, y, p)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mEgoData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mData\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medge_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medge_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \u001B[0;31m# self.ego_index = ego_index\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'numel'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import ToSparseTensor\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader.utils import filter_data\n",
    "\n",
    "batch_data = Batch.from_data_list([ego_data, ego_data])\n",
    "\n",
    "# batch_data = filter_data(data, n_id, batch_edge[0], batch_edge[1], e_id)\n",
    "# batch_data.batch_size = batch_node.numel()\n",
    "print(batch_data)\n",
    "# to_sparse = ToSparseTensor()\n",
    "# to_sparse(batch_data)\n",
    "# batch_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1439,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.ego_ptr = (batch_data.ego_ptr + batch_data.ptr[:-1])\n",
    "xr = batch_data.x[batch_data.ego_ptr]\n",
    "torch.equal(xr[0], xr[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "outputs": [
    {
     "data": {
      "text/plain": "SparseTensor(row=tensor([    0,     0,     0,  ..., 89249, 89249, 89249]),\n             col=tensor([    1,  1694,  2569,  ..., 28631, 52736, 78119]),\n             size=(89250, 89250), nnz=899756, density=0.01%)"
     },
     "execution_count": 1486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseTensor.from_edge_index(data.edge_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.rand(2,5)\n",
    "index = torch.LongTensor([2,3])\n",
    "print(x)\n",
    "\n",
    "# 如果想在x的第一个维度上选择x[2]和x[0]\n",
    "y = torch.index_select(x, dim=1, index=index)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 2, 3, 5]), tensor([6, 5, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "def unique(x, dim=-1):\n",
    "    unique, inverse = torch.unique(x, return_inverse=True, dim=dim)\n",
    "    perm = torch.arange(inverse.size(dim), dtype=inverse.dtype, device=inverse.device)\n",
    "    inverse, perm = inverse.flip([dim]), perm.flip([dim])\n",
    "    return unique, inverse.new_empty(unique.size(dim)).scatter_(dim, inverse, perm)\n",
    "\n",
    "t = torch.tensor([3, 3 ,5, 5, 3, 2, 1])\n",
    "print(unique(t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 3, 2, 2]) tensor([6, 5, 4, 3, 2, 1, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([1, 2, 3, 5]), tensor([6, 5, 0, 2]))"
     },
     "execution_count": 1270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _unique(x, dim=-1):\n",
    "    unique, inverse = torch.unique(x, return_inverse=True, dim=dim)\n",
    "    perm = torch.arange(inverse.size(dim), dtype=inverse.dtype, device=inverse.device)\n",
    "    inverse, perm = inverse.flip([dim]), perm.flip([dim])\n",
    "    return unique, inverse.new_empty(unique.size(dim)).scatter_(dim, inverse, perm)\n",
    "\n",
    "t = torch.tensor([3, 3 ,5, 5, 3, 2, 1])\n",
    "\n",
    "dim = -1\n",
    "unique, inverse = torch.unique(t, return_inverse=True, dim=dim)\n",
    "perm = torch.arange(inverse.size(dim), dtype=inverse.dtype, device=inverse.device)\n",
    "inverse, perm = inverse.flip([dim]), perm.flip([dim])\n",
    "print(inverse, perm)\n",
    "_unique(t)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "t_, inv = torch.unique(t, return_inverse=True)\n",
    "print(inv)\n",
    "scatter(t, inv, dim=-1, reduce='sum')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:29726 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1469-c5e589cf147c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0ma\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:29726 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "torch.cat(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "outputs": [
    {
     "data": {
      "text/plain": "(368, 368)"
     },
     "execution_count": 1489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_data.adj_t = SparseTensor(\n",
    "#             row=batch_data.edge_index[1], col=batch_data.edge_index[0],\n",
    "#             sparse_sizes=batch_data.edge_stores[0].size()[::-1],\n",
    "#             is_sorted=True)\n",
    "# delattr(batch_data, 'edge_index')\n",
    "batch_data.adj_t.sparse_sizes()[::-1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1494,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1494-35fdfef3d48a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.tensor([[torch.tensor([0]), torch.tensor([3])], [torch.tensor([0]), torch.tensor([])]]).t()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-pytorch-gpu-py",
   "language": "python",
   "display_name": "Python [conda env:pytorch-gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}